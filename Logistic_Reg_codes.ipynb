{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION CASE STUDY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets start buliding the model\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg=LogisticRegression()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\singh\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-de5dbf11c07b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# And define the DV and IVs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"DaysSinceLastERVisit\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Narcotics\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Pain\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"TotalVisits\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PoorCare'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_final' is not defined"
     ]
    }
   ],
   "source": [
    "#similar to what we do n Linear regression we split the data into 2 parts..\n",
    "# And define the DV and IVs.\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_final[[\"DaysSinceLastERVisit\",\"Narcotics\",\"Pain\",\"TotalVisits\",True]],df_final['PoorCare'],test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoorCare</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0      0  1\n",
       "PoorCare       \n",
       "0         70  1\n",
       "1         11  9"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict on train data\n",
    "log_reg.fit(X_train,y_train)\n",
    "pred_train=log_reg.predict(X_train)\n",
    "pd.crosstab(y_train,pred_train)\n",
    "#accuracy=(70+9)/(70+9+11+1)\n",
    "#accuracy=86% on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred=log_reg.predict(X_test)#to get the output as 1's and 0's\n",
    "#by assuming t=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoorCare</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0      0  1\n",
       "PoorCare       \n",
       "0         23  4\n",
       "1          6  7"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix\n",
    "pd.crosstab(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.75\n",
      "error is 0.25\n",
      "the coefficients are [[ -2.34777294e-03   1.32117981e-01  -3.96613720e-02   5.94438370e-02\n",
      "    2.74713754e+00]]\n",
      "the intercept is [-1.52482037]\n"
     ]
    }
   ],
   "source": [
    "TP=7.0\n",
    "TN=23.0\n",
    "FP=4.0\n",
    "FN=6.0\n",
    "#error on the test data\n",
    "accuracy= float((TP+TN)/len(y_test))\n",
    "error=float((FP+FN)/len(y_test))\n",
    "print (\"Accuracy is \")+ str(accuracy)\n",
    "print (\"error is \")+ str(error)\n",
    "#print the coefficients\n",
    "print \"the coefficients are \"+ str(log_reg.coef_)\n",
    "print \"the intercept is \"+ str(log_reg.intercept_)\n",
    "#we have got 75% accuracy, better than the baseline model\n",
    "#similarly you can also calulate Sensitivity, Specificity and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Also comparing the accuracy of Train 86% and test 75% , it looks like we have silghtly overfit..\n",
    "# we can try to exclude some variables to get a better fit and reduce complexity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so fro above we get the equation of the Logistic regression\n",
    "#\n",
    "#P(y=1)= 1/ 1+e^(-1.52 -0.0023*DaysSinceLastERVisit+0.132*Narcotics-... and so on.)\n",
    "#INTERPRETATION OF THE MODEL-----------------------------------------------------\n",
    "#This means that icreasing DaysSinceLastERVisit,Pain (negative coefficient) takes you towards P(y=0) or PoorCare(0). \n",
    "#This maskes sense as well\n",
    "#Similarly higher value of Narcotics & TotalVisits would means you are more likely to have \n",
    "#reveived Poor Care. This too makes sense( read the variable description at the begining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1=log_reg.predict_proba(X_test)# to get probabilities instead of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.05, 1.05)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAD9CAYAAABX/HkoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef54630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lets us look at ROC curve\n",
    "#dont worry about the codes fro ROC curve.. You dont have to remember it.. Just know its application\n",
    "#to read more about ROC curve refer---->\n",
    "# http://blog.yhat.com/posts/roc-curves.html or http://www.dataschool.io/roc-curves-and-auc-explained/\n",
    "fpr, tpr, threshold= metrics.roc_curve(y_test, pred1[:,1]) #pass the actual test DV and \n",
    "#predicted probability (only P(y)=1)\n",
    "#**********DRAW ROC CURVE**************\n",
    "plt.plot(fpr, tpr, label='ROC curve', color='b')\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.783475783476\n"
     ]
    }
   ],
   "source": [
    "AUC= metrics.auc(fpr,tpr) # to get the area under the Curve\n",
    "print AUC #78% which is good. this means model is covering 78% of the datapoints well. Best value is 1\n",
    "#worst is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoorCare</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0      0  1\n",
       "PoorCare       \n",
       "0         23  4\n",
       "1          5  8"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CONVERT THE PROBABILITITS TO 1'S AND 0'S APPLYING OUT OWN THRESHOLD\n",
    "#HERE 0.40\n",
    "x=[]\n",
    "for i in range(len(pred1[:,1])):\n",
    "    if (pred1[:,1][i]>=0.4):\n",
    "        x.append(1)\n",
    "    else:\n",
    "        x.append(0)\n",
    "x=np.array(x)\n",
    "pd.crosstab(y_test,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.775\n",
      "0.225\n"
     ]
    }
   ],
   "source": [
    "#output 2\n",
    "TP=8.0\n",
    "TN=23.0\n",
    "FP=4.0\n",
    "FN=5.0\n",
    "#error on the test data\n",
    "accuracy= float((TP+TN)/len(y_test))\n",
    "error=float((FP+FN)/len(y_test))\n",
    "print accuracy\n",
    "print error\n",
    "#slightly better than before\n",
    "#similarly you can also calulate Sensitivity, Specificity and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets see the P values to verify if we have used all Good variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.discrete.discrete_model as sm\n",
    "from statsmodels.api import add_constant\n",
    "X2 = add_constant(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = sm.Logit(y_train, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.370628\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:               PoorCare   No. Observations:                   91\n",
      "Model:                          Logit   Df Residuals:                       85\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Mon, 11 Dec 2017   Pseudo R-squ.:                  0.2962\n",
      "Time:                        00:36:26   Log-Likelihood:                -33.727\n",
      "converged:                       True   LL-Null:                       -47.923\n",
      "                                        LLR p-value:                 3.050e-05\n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "----------------------------------------------------------------------------------------\n",
      "const                   -1.5322      0.805     -1.904      0.057        -3.109     0.045\n",
      "DaysSinceLastERVisit    -0.0023      0.001     -1.903      0.057        -0.005  6.97e-05\n",
      "Narcotics                0.1324      0.044      3.040      0.002         0.047     0.218\n",
      "Pain                    -0.0398      0.027     -1.472      0.141        -0.093     0.013\n",
      "TotalVisits              0.0597      0.036      1.662      0.096        -0.011     0.130\n",
      "True                     2.7626      1.409      1.961      0.050         0.001     5.524\n",
      "========================================================================================\n"
     ]
    }
   ],
   "source": [
    "result = logit.fit()\n",
    "print result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see from abve that Pain and Total Visits are not imprtant pain 14% and Total Visits 9.6%\n",
    "#let lets exclude them and recheck the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_final[[\"DaysSinceLastERVisit\",\"Narcotics\",True]],df_final['PoorCare'],test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000000000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg=LogisticRegression(C = 1e9)\n",
    "log_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoorCare</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0      0  1\n",
       "PoorCare       \n",
       "0         70  1\n",
       "1         11  9"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict on train data\n",
    "pred_train=log_reg.predict(X_train)\n",
    "pd.crosstab(y_train,pred_train)\n",
    "#accuracy=(70+9)/(70+9+11+1)\n",
    "#accuracy=86% on training data\n",
    "#similarly you can also calulate Sensitivity, Specificity and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.8\n"
     ]
    }
   ],
   "source": [
    "#predict on test\n",
    "pred=log_reg.predict(X_test)#to get the output as 1's and 0's\n",
    "#by assuming t=0.5\n",
    "#confusion matrix\n",
    "pd.crosstab(y_test,pred)\n",
    "#test data accuracy= 27+5=32\n",
    "print \"accuracy is \"+ str(32.0/40) #80% better than previous result and very close to train data accuracy (86%).\n",
    "#this is random variation.\n",
    "#hence overfitting has been removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#so final model will have only DaysSinceLastERVisit\",\"Narcotics\" & \n",
    "#StartedOnCombination(dummy of this) as the INDEPENDENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#HOPE THIS CASE STUDY HELPED YOU. THIS IS HOW TO PROCEED WITH ANY DATA.. ONE STEP AT A TIME.\n",
    "# NURTURING YOUR DATA SCIECNE JOURNEY AND HONING YOUR SKILLS.\n",
    "# MAIL AT BANGALOREDATASCIENCEACADEMY@GMAIL.COM FOR MORE SUCH CASE STUDIES"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
